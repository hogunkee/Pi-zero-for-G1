"""
LeRobot ë°ì´í„°ì…‹ ë³‘í•© ìŠ¤í¬ë¦½íŠ¸
í•˜ë‚˜ì˜ í´ë” ì•ˆì— ìˆëŠ” ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
"""

import shutil
import json
import re
import os
from pathlib import Path
import pyarrow.parquet as pq
import pyarrow as pa
import numpy as np
import pandas as pd
from tqdm import tqdm


def unit_scale(unit: str) -> float:
    """ì‹œê°„ ë‹¨ìœ„ë¥¼ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜"""
    return {"s": 1.0, "ms": 1e3, "us": 1e6, "ns": 1e9}[unit]


def detect_timestamp_cols(df: pd.DataFrame) -> list:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ìë™ ê°ì§€"""
    cols = []
    for c in df.columns:
        cl = str(c).lower()
        if "timestamp" in cl or cl.endswith("_ts"):
            cols.append(c)
    return cols


def make_uniform(n: int, fps: float, unit: str, base=0.0, as_int=False):
    """FPSì— ë§ëŠ” ê· ì¼í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
    t = (np.arange(n, dtype=np.float64) / float(fps)) * unit_scale(unit)
    t += float(base)
    if as_int:
        t = np.rint(t).astype(np.int64)
    return t


def filter_observation_state(state_value):
    """observation.stateì—ì„œ ì• 15ê°œ dof ì œê±° (index 15ë¶€í„°ë§Œ ìœ ì§€)"""
    if isinstance(state_value, (list, np.ndarray)):
        return state_value[15:]
    return state_value


def load_metadata(meta_dir):
    """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    meta_dir = Path(meta_dir)
    metadata = {}

    # info.json ë¡œë“œ
    info_path = meta_dir / "info.json"
    if info_path.exists():
        with open(info_path, 'r') as f:
            metadata['info'] = json.load(f)

    # episodes.jsonl ë¡œë“œ
    episodes_path = meta_dir / "episodes.jsonl"
    if episodes_path.exists():
        episodes = []
        with open(episodes_path, 'r') as f:
            for line in f:
                episodes.append(json.loads(line))
        metadata['episodes'] = episodes

    # tasks.jsonl ë¡œë“œ (ìˆì„ ê²½ìš°)
    tasks_path = meta_dir / "tasks.jsonl"
    if tasks_path.exists():
        tasks = []
        with open(tasks_path, 'r') as f:
            for line in f:
                tasks.append(json.loads(line))
        metadata['tasks'] = tasks

    # stats.json ë¡œë“œ (ìˆì„ ê²½ìš°)
    stats_path = meta_dir / "stats.json"
    if stats_path.exists():
        with open(stats_path, 'r') as f:
            metadata['stats'] = json.load(f)

    # modality.json ë¡œë“œ (ìˆì„ ê²½ìš°)
    modality_path = meta_dir / "modality.json"
    if modality_path.exists():
        with open(modality_path, 'r') as f:
            metadata['modality'] = json.load(f)

    return metadata


def get_last_index_from_parquet(data_dir):
    """ë°ì´í„°ì…‹ì˜ ë§ˆì§€ë§‰ index ê°’ì„ ì°¾ê¸°"""
    data_dir = Path(data_dir)
    parquet_dir = data_dir / "data" / "chunk-000"

    if not parquet_dir.exists():
        return -1

    parquet_files = sorted(parquet_dir.glob("*.parquet"))
    if not parquet_files:
        return -1

    # ë§ˆì§€ë§‰ parquet íŒŒì¼ ì½ê¸°
    last_file = parquet_files[-1]
    table = pq.read_table(last_file)

    # index ì»¬ëŸ¼ì˜ ë§ˆì§€ë§‰ ê°’ ë°˜í™˜
    if 'index' in table.column_names:
        index_column = table.column('index').to_pylist()
        return max(index_column) if index_column else -1

    return -1


def load_task_dict(tasks_decompose_path):
    """tasks_decompose.jsonl ë¡œë“œí•˜ì—¬ task_dict ìƒì„±"""
    task_dict = {}
    tasks_decompose_path = Path(tasks_decompose_path)

    if not tasks_decompose_path.exists():
        print(f"  âš  tasks_decompose.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tasks_decompose_path}")
        return task_dict

    with open(tasks_decompose_path, 'r', encoding='utf-8') as f:
        for line in f:
            x = json.loads(line)
            task_dict[x["task"]] = x["task_index"]

    return task_dict


def get_task_index_from_folder_name(folder_name, task_dict):
    """í´ë” ì´ë¦„ìœ¼ë¡œë¶€í„° task_index ì°¾ê¸°"""
    # ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­
    task_name = folder_name.replace("_", " ")

    if task_name in task_dict:
        return task_dict[task_name]

    # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ None ë°˜í™˜
    return None


def get_state_dimension(parquet_file):
    """parquet íŒŒì¼ì—ì„œ observation.stateì˜ dimension í™•ì¸"""
    table = pq.read_table(parquet_file)
    df = table.to_pandas()

    if "observation.state" in df.columns:
        first_state = df["observation.state"].iloc[0]
        if isinstance(first_state, (list, np.ndarray)):
            return len(first_state)

    return None


def merge_datasets(datasets_dir, output_dir, tasks_decompose_path="tasks_decompose.jsonl",
                   fps=None, timestamp_unit="s", timestamp_as_int=False):
    """
    í•˜ë‚˜ì˜ í´ë” ë‚´ ì—¬ëŸ¬ LeRobot ë°ì´í„°ì…‹ì„ ë³‘í•©

    Args:
        datasets_dir: ì—¬ëŸ¬ ë°ì´í„°ì…‹ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œ
        output_dir: ì¶œë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ
        tasks_decompose_path: tasks_decompose.jsonl íŒŒì¼ ê²½ë¡œ
        fps: FPS (íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë°ìš©, Noneì´ë©´ ë¦¬íƒ€ì´ë° ì•ˆ í•¨)
        timestamp_unit: íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ìœ„ ("s", "ms", "us", "ns")
        timestamp_as_int: íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ int64ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
    """
    datasets_dir = Path(datasets_dir)
    output_dir = Path(output_dir)

    print("=" * 70)
    print("LeRobot ë°ì´í„°ì…‹ ë³‘í•© ì‹œì‘")
    print("=" * 70)

    # tasks_decompose.jsonl ë¡œë“œ
    print(f"\n[1] tasks_decompose.jsonl ë¡œë“œ ì¤‘...")
    task_dict = load_task_dict(tasks_decompose_path)
    print(f"  âœ“ {len(task_dict)}ê°œ íƒœìŠ¤í¬ ì •ì˜ ë¡œë“œ")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / "data" / "chunk-000").mkdir(parents=True, exist_ok=True)
    (output_dir / "meta").mkdir(parents=True, exist_ok=True)
    (output_dir / "videos" / "chunk-000").mkdir(parents=True, exist_ok=True)

    # ë°ì´í„°ì…‹ í´ë”ë“¤ ì°¾ê¸°
    print(f"\n[2] ë°ì´í„°ì…‹ ê²€ìƒ‰ ì¤‘...")
    dataset_folders = []
    for item in sorted(os.listdir(datasets_dir)):
        item_path = datasets_dir / item
        if item_path.is_dir():
            # meta í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
            if (item_path / "meta" / "info.json").exists():
                dataset_folders.append(item)

    print(f"  âœ“ {len(dataset_folders)}ê°œ ë°ì´í„°ì…‹ ë°œê²¬: {dataset_folders}")

    if len(dataset_folders) == 0:
        raise ValueError(f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {datasets_dir}")

    # ë³‘í•© ì¤€ë¹„
    total_episodes = 0
    total_frames = 0
    total_videos = 0
    merged_episodes = []
    merged_tasks = None
    merged_modality = None
    merged_stats = None

    current_last_index = -1
    parquet_output_count = 0
    video_output_count = 0

    # ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
    print(f"\n[3] ë°ì´í„°ì…‹ ë³‘í•© ì¤‘...")
    for dataset_idx, dataset_name in enumerate(dataset_folders):
        dataset_path = datasets_dir / dataset_name
        print(f"\n  [{dataset_idx + 1}/{len(dataset_folders)}] '{dataset_name}' ì²˜ë¦¬ ì¤‘...")

        # task_index ì°¾ê¸°
        task_index = get_task_index_from_folder_name(dataset_name, task_dict)
        if task_index is None:
            print(f"    âš  task_indexë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {dataset_name}")
            continue
        print(f"    âœ“ task_index: {task_index}")

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        meta = load_metadata(dataset_path / "meta")
        num_episodes = meta['info']['total_episodes']
        num_frames = meta['info']['total_frames']

        # Parquet íŒŒì¼ ì²˜ë¦¬
        src_parquet_dir = dataset_path / "data" / "chunk-000"
        dst_parquet_dir = output_dir / "data" / "chunk-000"

        if not src_parquet_dir.exists():
            print(f"    âš  parquet íŒŒì¼ì´ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤")
            continue

        parquet_files = sorted(src_parquet_dir.glob("*.parquet"))
        print(f"    ğŸ“¦ {len(parquet_files)}ê°œ parquet íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

        # state dimension í™•ì¸ (ì²« ë²ˆì§¸ íŒŒì¼ë¡œ)
        state_dim = None
        if parquet_files:
            state_dim = get_state_dimension(parquet_files[0])
            if state_dim is not None:
                print(f"    ğŸ“Š observation.state dimension: {state_dim}")
                if state_dim == 43:
                    print(f"    âœ‚ï¸ state í•„í„°ë§ ì ìš© (ì• 15ê°œ ì œê±°)")

        for pf in tqdm(parquet_files, desc=f"    Processing", leave=False):
            # Parquet íŒŒì¼ ì½ê¸°
            table = pq.read_table(pf)
            df = table.to_pandas()

            # episode_indexì™€ index ì¡°ì •
            if 'episode_index' in df.columns:
                df['episode_index'] = df['episode_index'] + total_episodes
            if 'index' in df.columns:
                df['index'] = df['index'] + current_last_index + 1

            # task_index ì„¤ì •
            if 'task_index' in df.columns:
                df['task_index'] = task_index

            # state í•„í„°ë§ (dimensionì´ 43ì¼ ë•Œë§Œ)
            if state_dim == 43 and "observation.state" in df.columns:
                df["observation.state"] = df["observation.state"].apply(filter_observation_state)

            # íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë° (fpsê°€ ì§€ì •ëœ ê²½ìš°)
            if fps is not None:
                ts_cols = detect_timestamp_cols(df)
                if ts_cols:
                    for col in ts_cols:
                        df[col] = make_uniform(len(df), fps, timestamp_unit, base=0.0, as_int=timestamp_as_int)

            # ìƒˆë¡œìš´ íŒŒì¼ëª… ìƒì„±
            new_name = pf.name
            match = re.search(r'episode_(\d+)', new_name)
            if match:
                ep_num = int(match.group(1))
                new_ep_num = ep_num + total_episodes
                old_pattern = match.group(0)
                new_pattern = f"episode_{new_ep_num:0{len(match.group(1))}d}"
                new_name = new_name.replace(old_pattern, new_pattern)

            # ì €ì¥
            table = pa.Table.from_pandas(df)
            pq.write_table(table, dst_parquet_dir / new_name)
            parquet_output_count += 1

        print(f"    âœ“ {len(parquet_files)}ê°œ parquet íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")

        # í˜„ì¬ ë°ì´í„°ì…‹ì˜ ë§ˆì§€ë§‰ index ì—…ë°ì´íŠ¸
        dataset_last_index = get_last_index_from_parquet(dataset_path)
        if dataset_last_index >= 0:
            current_last_index = current_last_index + 1 + dataset_last_index

        # Episodes ë³‘í•©
        if 'episodes' in meta:
            for ep in meta['episodes']:
                ep_adjusted = ep.copy()
                ep_adjusted['episode_index'] = ep['episode_index'] + total_episodes
                # task_index ì¶”ê°€
                if 'task_index' not in ep_adjusted:
                    ep_adjusted['task_index'] = task_index
                merged_episodes.append(ep_adjusted)

        # Tasks ë³‘í•© (ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©)
        if merged_tasks is None and 'tasks' in meta:
            merged_tasks = meta['tasks']

        # Modality ë³‘í•© (ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©, state dimension 43ì´ë©´ ìˆ˜ì •)
        if merged_modality is None and 'modality' in meta:
            merged_modality = meta['modality'].copy()
            # state dimensionì´ 43ì´ë©´ modality ìˆ˜ì •
            if state_dim == 43 and 'state' in merged_modality:
                new_state = {
                    "left_arm": {"start": 0, "end": 7},
                    "right_arm": {"start": 7, "end": 14},
                    "left_hand": {"start": 14, "end": 21},
                    "right_hand": {"start": 21, "end": 28}
                }
                merged_modality["state"] = new_state

        # Stats ë³‘í•© (ì²« ë²ˆì§¸ ê²ƒë§Œ ì‚¬ìš©)
        if merged_stats is None and 'stats' in meta:
            merged_stats = meta['stats']

        # ë¹„ë””ì˜¤ íŒŒì¼ ë³µì‚¬
        video_src = dataset_path / "videos" / "chunk-000"
        video_dst = output_dir / "videos" / "chunk-000"

        video_count = 0
        if video_src.exists():
            for video_file in video_src.glob("*.mp4"):
                new_name = video_file.name

                # episode_XXXX í˜•íƒœì˜ ë²ˆí˜¸ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ì•„ì„œ ì¡°ì •
                match = re.search(r'episode_(\d+)', new_name)
                if match:
                    ep_num = int(match.group(1))
                    new_ep_num = ep_num + total_episodes
                    old_pattern = match.group(0)
                    new_pattern = f"episode_{new_ep_num:0{len(match.group(1))}d}"
                    new_name = new_name.replace(old_pattern, new_pattern)

                shutil.copy2(video_file, video_dst / new_name)
                video_count += 1
                video_output_count += 1

        if video_count > 0:
            print(f"    ğŸ“¹ {video_count}ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")

        # ëˆ„ì 
        total_episodes += num_episodes
        total_frames += num_frames
        total_videos += video_count

    # ë³‘í•©ëœ ë©”íƒ€ë°ì´í„° ìƒì„±
    print(f"\n[4] ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")

    # info.json ìƒì„± (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê¸°ë°˜)
    first_dataset = datasets_dir / dataset_folders[0]
    first_meta = load_metadata(first_dataset / "meta")

    merged_info = first_meta['info'].copy()
    merged_info['total_episodes'] = total_episodes
    merged_info['total_frames'] = total_frames
    merged_info['total_videos'] = total_videos
    merged_info['total_chunks'] = 1
    merged_info['total_tasks'] = len(task_dict)

    # splits ì •ë³´ ì—…ë°ì´íŠ¸
    if 'splits' in merged_info:
        for split_name, split_value in merged_info['splits'].items():
            if isinstance(split_value, str) and ':' in split_value:
                if split_name == 'train':
                    merged_info['splits'][split_name] = f"0:{total_episodes}"
            elif isinstance(split_value, (int, float)):
                if split_name == 'train':
                    merged_info['splits'][split_name] = total_episodes

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    print(f"\n[5] ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")

    with open(output_dir / "meta" / "info.json", 'w') as f:
        json.dump(merged_info, f, indent=2)
    print(f"  âœ“ info.json ì €ì¥")

    if merged_episodes:
        with open(output_dir / "meta" / "episodes.jsonl", 'w') as f:
            for ep in merged_episodes:
                f.write(json.dumps(ep) + '\n')
        print(f"  âœ“ episodes.jsonl ì €ì¥")

    if merged_tasks:
        with open(output_dir / "meta" / "tasks.jsonl", 'w') as f:
            for task in merged_tasks:
                f.write(json.dumps(task) + '\n')
        print(f"  âœ“ tasks.jsonl ì €ì¥")

    if merged_stats:
        with open(output_dir / "meta" / "stats.json", 'w') as f:
            json.dump(merged_stats, f, indent=2)
        print(f"  âœ“ stats.json ì €ì¥")

    if merged_modality:
        with open(output_dir / "meta" / "modality.json", 'w') as f:
            json.dump(merged_modality, f, indent=2)
        print(f"  âœ“ modality.json ì €ì¥")

    print("\n" + "=" * 70)
    print("âœ… ë³‘í•© ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“Š ë³‘í•© ê²°ê³¼:")
    print(f"  - ë³‘í•©ëœ ë°ì´í„°ì…‹: {len(dataset_folders)}ê°œ")
    print(f"  - ì´ ì—í”¼ì†Œë“œ: {total_episodes}")
    print(f"  - ì´ í”„ë ˆì„: {total_frames}")
    print(f"  - Parquet íŒŒì¼: {parquet_output_count}ê°œ")
    print(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {video_output_count}ê°œ")
    print(f"  - ì¶œë ¥ ê²½ë¡œ: {output_dir}")
    print()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LeRobot ë°ì´í„°ì…‹ ë³‘í•© (ì—¬ëŸ¬ ë°ì´í„°ì…‹)")
    parser.add_argument("--datasets_dir", type=str, required=True,
                        help="ì—¬ëŸ¬ ë°ì´í„°ì…‹ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="ì¶œë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--tasks_decompose", type=str, default="tasks_decompose.jsonl",
                        help="tasks_decompose.jsonl íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: tasks_decompose.jsonl)")

    # íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë° ì˜µì…˜
    parser.add_argument("--fps", type=float, default=50,
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë°ì„ ìœ„í•œ FPS (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë¦¬íƒ€ì´ë° ì•ˆ í•¨)")
    parser.add_argument("--timestamp_unit", choices=["s", "ms", "us", "ns"], default="s",
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ìœ„ (ê¸°ë³¸ê°’: s)")
    parser.add_argument("--timestamp_as_int", action="store_true",
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ int64ë¡œ ì €ì¥ (ê¸°ë³¸ê°’: float64)")

    args = parser.parse_args()

    merge_datasets(
        args.datasets_dir,
        args.output_dir,
        tasks_decompose_path=args.tasks_decompose,
        fps=args.fps,
        timestamp_unit=args.timestamp_unit,
        timestamp_as_int=args.timestamp_as_int
    )

# python merge_datasets.py \
#   --datasets_dir /data1/hogun/dataset/1205_TaskDecompose \
#   --output_dir /data1/hogun/dataset/merged_output \
#   --tasks_decompose tasks_decompose.jsonl \
#   --fps 30