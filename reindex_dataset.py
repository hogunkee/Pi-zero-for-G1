"""
LeRobot ë°ì´í„°ì…‹ ì¬ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸
ë³€í™”ì ì´ ìˆëŠ” ë°ì´í„°ë§Œ ë‚¨ê¸´ í›„ ì¸ë±ìŠ¤ë¥¼ ì—°ì†ì ìœ¼ë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
"""

import argparse
import json
import re
import shutil
from pathlib import Path
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa


def extract_episode_number(filename):
    """íŒŒì¼ëª…ì—ì„œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ì¶œ"""
    match = re.search(r'episode_(\d+)', filename)
    if match:
        return int(match.group(1))
    return None


def reindex_dataset(input_dir, output_dir, dry_run=False):
    """
    ë°ì´í„°ì…‹ì„ ì¬ì¸ë±ì‹±í•©ë‹ˆë‹¤.

    Args:
        input_dir: ì…ë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ (smooth_phase.py ì¶œë ¥)
        output_dir: ì¬ì¸ë±ì‹±ëœ ë°ì´í„°ì…‹ì„ ì €ì¥í•  ê²½ë¡œ
        dry_run: Trueë©´ ì €ì¥í•˜ì§€ ì•Šê³  í†µê³„ë§Œ ì¶œë ¥
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)

    print("=" * 70)
    print("LeRobot ë°ì´í„°ì…‹ ì¬ì¸ë±ì‹± ì‹œì‘")
    print("=" * 70)
    print(f"ğŸ“ ì…ë ¥: {input_dir}")
    print(f"ğŸ“ ì¶œë ¥: {output_dir}")
    if dry_run:
        print("ğŸ’¡ DRY RUN ëª¨ë“œ (ì €ì¥ ì•ˆ í•¨)")
    print()

    # ë””ë ‰í† ë¦¬ í™•ì¸
    parquet_dir = input_dir / "data" / "chunk-000"
    video_dir = input_dir / "videos" / "chunk-000"
    meta_dir = input_dir / "meta"

    if not parquet_dir.exists():
        raise FileNotFoundError(f"Parquet ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parquet_dir}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if not dry_run:
        (output_dir / "data" / "chunk-000").mkdir(parents=True, exist_ok=True)
        (output_dir / "videos" / "chunk-000").mkdir(parents=True, exist_ok=True)
        (output_dir / "meta").mkdir(parents=True, exist_ok=True)

    # 1. Parquet íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬
    print("[1/4] Parquet íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    parquet_files = sorted(parquet_dir.glob("*.parquet"))
    print(f"  âœ“ {len(parquet_files)}ê°œ íŒŒì¼ ë°œê²¬")

    # 2. ì—í”¼ì†Œë“œ ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
    print("\n[2/4] ì—í”¼ì†Œë“œ ì¬ì¸ë±ì‹± ì¤‘...")
    old_to_new_episode = {}
    new_episode_idx = 0

    for pf in parquet_files:
        old_ep_num = extract_episode_number(pf.name)
        if old_ep_num is not None and old_ep_num not in old_to_new_episode:
            old_to_new_episode[old_ep_num] = new_episode_idx
            new_episode_idx += 1

    print(f"  âœ“ ê¸°ì¡´ ì—í”¼ì†Œë“œ: {len(old_to_new_episode)}ê°œ")
    print(f"  âœ“ ìƒˆ ì—í”¼ì†Œë“œ: {new_episode_idx}ê°œ")

    # 3. Parquet íŒŒì¼ ì¬ì¸ë±ì‹± ë° ì €ì¥
    print("\n[3/4] Parquet íŒŒì¼ ì¬ì¸ë±ì‹± ì¤‘...")

    total_frames = 0
    new_index = 0
    episode_info = {}  # {new_episode_idx: {'length': N, 'old_idx': M}}

    for pf in sorted(parquet_files):
        old_ep_num = extract_episode_number(pf.name)
        if old_ep_num is None:
            print(f"  âš ï¸  íŒŒì¼ëª…ì—ì„œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pf.name}")
            continue

        new_ep_num = old_to_new_episode[old_ep_num]

        # Parquet íŒŒì¼ ì½ê¸°
        table = pq.read_table(pf)
        df = table.to_pandas()

        # í”„ë ˆì„ ìˆ˜ ì¹´ìš´íŠ¸
        num_frames = len(df)
        total_frames += num_frames

        # ì—í”¼ì†Œë“œ ì •ë³´ ì €ì¥
        if new_ep_num not in episode_info:
            episode_info[new_ep_num] = {
                'length': num_frames,
                'old_idx': old_ep_num,
                'start_index': new_index
            }

        # episode_index ì¬ì„¤ì •
        if 'episode_index' in df.columns:
            df['episode_index'] = new_ep_num

        # index ì¬ì„¤ì • (ì—°ì†ì ìœ¼ë¡œ)
        if 'index' in df.columns:
            df['index'] = list(range(new_index, new_index + num_frames))

        new_index += num_frames

        # ìƒˆ íŒŒì¼ëª… ìƒì„±
        new_filename = f"episode_{new_ep_num:06d}.parquet"

        # ì €ì¥
        if not dry_run:
            table = pa.Table.from_pandas(df)
            pq.write_table(table, output_dir / "data" / "chunk-000" / new_filename)

        print(f"  âœ“ episode_{old_ep_num:06d}.parquet -> {new_filename} ({num_frames} frames)")

    print(f"\n  ğŸ“Š ì´ {total_frames}ê°œ í”„ë ˆì„")

    # 4. ë¹„ë””ì˜¤ íŒŒì¼ ë³µì‚¬ (ì—í”¼ì†Œë“œ ë²ˆí˜¸ ë§¤ì¹­)
    print("\n[4/4] ë¹„ë””ì˜¤ íŒŒì¼ ë³µì‚¬ ì¤‘...")

    if video_dir.exists():
        video_files = list(video_dir.glob("*.mp4"))
        copied_videos = 0

        for vf in video_files:
            old_ep_num = extract_episode_number(vf.name)
            if old_ep_num is None:
                print(f"  âš ï¸  íŒŒì¼ëª…ì—ì„œ ì—í”¼ì†Œë“œ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {vf.name}")
                continue

            # ë§¤í•‘ì— ì—†ëŠ” ì—í”¼ì†Œë“œëŠ” ê±´ë„ˆëœ€ (smooth_phaseì—ì„œ ì œê±°ëœ ê²ƒ)
            if old_ep_num not in old_to_new_episode:
                print(f"  â­ï¸  ê±´ë„ˆëœ€ (parquet ì—†ìŒ): {vf.name}")
                continue

            new_ep_num = old_to_new_episode[old_ep_num]

            # ìƒˆ íŒŒì¼ëª… ìƒì„±
            new_name = re.sub(
                r'episode_(\d+)',
                f'episode_{new_ep_num:06d}',
                vf.name
            )

            if not dry_run:
                shutil.copy2(vf, output_dir / "videos" / "chunk-000" / new_name)

            copied_videos += 1
            print(f"  âœ“ {vf.name} -> {new_name}")

        print(f"\n  ğŸ“Š {copied_videos}ê°œ ë¹„ë””ì˜¤ ë³µì‚¬ ì™„ë£Œ")
    else:
        print("  âš ï¸  ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # 5. ë©”íƒ€ë°ì´í„° ìƒì„±
    print("\n[5/5] ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")

    if meta_dir.exists():
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        info_path = meta_dir / "info.json"
        if info_path.exists():
            with open(info_path, 'r') as f:
                info = json.load(f)
        else:
            info = {}

        # episodes.jsonl ë¡œë“œ (ìˆìœ¼ë©´)
        episodes_path = meta_dir / "episodes.jsonl"
        old_episodes = []
        if episodes_path.exists():
            with open(episodes_path, 'r') as f:
                for line in f:
                    old_episodes.append(json.loads(line))

        # tasks.jsonl ë¡œë“œ (ìˆìœ¼ë©´)
        tasks_path = meta_dir / "tasks.jsonl"
        tasks = []
        if tasks_path.exists():
            with open(tasks_path, 'r') as f:
                for line in f:
                    tasks.append(json.loads(line))

        # info.json ì—…ë°ì´íŠ¸
        info['total_episodes'] = new_episode_idx
        info['total_videos'] = new_episode_idx
        info['total_frames'] = total_frames
        info['total_chunks'] = 1

        # splits ì •ë³´ ì—…ë°ì´íŠ¸
        if 'splits' in info:
            for split_name in info['splits']:
                if split_name == 'train':
                    if isinstance(info['splits'][split_name], str) and ':' in info['splits'][split_name]:
                        info['splits'][split_name] = f"0:{new_episode_idx}"
                    else:
                        info['splits'][split_name] = new_episode_idx

        # episodes.jsonl ì¬ìƒì„±
        new_episodes = []
        for new_ep_idx in sorted(episode_info.keys()):
            old_ep_idx = episode_info[new_ep_idx]['old_idx']

            # ê¸°ì¡´ ì—í”¼ì†Œë“œ ì •ë³´ ì°¾ê¸°
            old_ep_info = None
            for ep in old_episodes:
                if ep.get('episode_index') == old_ep_idx:
                    old_ep_info = ep.copy()
                    break

            if old_ep_info:
                old_ep_info['episode_index'] = new_ep_idx
                new_episodes.append(old_ep_info)
            else:
                # ê¸°ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ë§Œ ìƒì„±
                new_episodes.append({
                    'episode_index': new_ep_idx,
                    'length': episode_info[new_ep_idx]['length']
                })

        # ì €ì¥
        if not dry_run:
            # info.json ì €ì¥
            with open(output_dir / "meta" / "info.json", 'w') as f:
                json.dump(info, f, indent=2)
            print("  âœ“ info.json ì €ì¥")

            # episodes.jsonl ì €ì¥
            with open(output_dir / "meta" / "episodes.jsonl", 'w') as f:
                for ep in new_episodes:
                    f.write(json.dumps(ep) + '\n')
            print("  âœ“ episodes.jsonl ì €ì¥")

            # tasks.jsonl ë³µì‚¬ (ë³€ê²½ ì—†ìŒ)
            if tasks:
                with open(output_dir / "meta" / "tasks.jsonl", 'w') as f:
                    for task in tasks:
                        f.write(json.dumps(task) + '\n')
                print("  âœ“ tasks.jsonl ì €ì¥")

            # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ë³µì‚¬ (stats.json, modality.json ë“±)
            for meta_file in meta_dir.glob("*.json"):
                if meta_file.name not in ['info.json']:
                    shutil.copy2(meta_file, output_dir / "meta" / meta_file.name)
                    print(f"  âœ“ {meta_file.name} ë³µì‚¬")
    else:
        print("  âš ï¸  ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 70)
    print("âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“Š ìš”ì•½:")
    print(f"  - ì´ ì—í”¼ì†Œë“œ: {new_episode_idx}")
    print(f"  - ì´ í”„ë ˆì„: {total_frames}")
    print(f"  - Parquet íŒŒì¼: {len(parquet_files)}ê°œ")
    print(f"  - ì¶œë ¥ ê²½ë¡œ: {output_dir}")

    if old_to_new_episode:
        removed_count = max(old_to_new_episode.keys()) + 1 - len(old_to_new_episode)
        if removed_count > 0:
            print(f"  - ì œê±°ëœ ì—í”¼ì†Œë“œ: {removed_count}ê°œ (ë³€í™”ì  ì—†ìŒ)")
    print()


def main():
    parser = argparse.ArgumentParser(
        description="LeRobot ë°ì´í„°ì…‹ ì¬ì¸ë±ì‹± (smooth_phase ì´í›„)"
    )
    parser.add_argument(
        "input_dir",
        type=str,
        help="ì…ë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ (smooth_phase ì¶œë ¥)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: <input_dir>_reindexed)"
    )
    parser.add_argument(
        "--dry_run",
        action="store_true",
        help="ì‹¤ì œë¡œ ì €ì¥í•˜ì§€ ì•Šê³  í†µê³„ë§Œ ì¶œë ¥"
    )

    args = parser.parse_args()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = args.output_dir
    else:
        output_dir = str(Path(args.input_dir).resolve()) + "_reindexed"

    reindex_dataset(args.input_dir, output_dir, args.dry_run)


if __name__ == "__main__":
    main()
