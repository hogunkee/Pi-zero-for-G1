"""
LeRobot ë°ì´í„°ì…‹ ë³‘í•© ìŠ¤í¬ë¦½íŠ¸
ë‘ ê°œì˜ LeRobot ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
"""

import shutil
import json
import re
from pathlib import Path
import pyarrow.parquet as pq
import pyarrow as pa
import numpy as np
import pandas as pd


def unit_scale(unit: str) -> float:
    """ì‹œê°„ ë‹¨ìœ„ë¥¼ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜"""
    return {"s": 1.0, "ms": 1e3, "us": 1e6, "ns": 1e9}[unit]


def detect_timestamp_cols(df: pd.DataFrame) -> list:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ìë™ ê°ì§€"""
    cols = []
    for c in df.columns:
        cl = str(c).lower()
        if "timestamp" in cl or cl.endswith("_ts"):
            cols.append(c)
    return cols


def detect_task_cols(df: pd.DataFrame) -> list:
    """íƒœìŠ¤í¬ ì»¬ëŸ¼ ìë™ ê°ì§€"""
    cols = []
    for c in df.columns:
        cl = str(c).lower()
        if "task" in cl or cl.endswith("_ts"):
            cols.append(c)
    return cols


def make_uniform(n: int, fps: float, unit: str, base=0.0, as_int=False):
    """FPSì— ë§ëŠ” ê· ì¼í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
    t = (np.arange(n, dtype=np.float64) / float(fps)) * unit_scale(unit)
    t += float(base)
    if as_int:
        t = np.rint(t).astype(np.int64)
    return t


def load_metadata(meta_dir):
    """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    meta_dir = Path(meta_dir)
    metadata = {}
    
    # info.json ë¡œë“œ
    info_path = meta_dir / "info.json"
    if info_path.exists():
        with open(info_path, 'r') as f:
            metadata['info'] = json.load(f)
    
    # episodes.jsonl ë¡œë“œ
    episodes_path = meta_dir / "episodes.jsonl"
    if episodes_path.exists():
        episodes = []
        with open(episodes_path, 'r') as f:
            for line in f:
                episodes.append(json.loads(line))
        metadata['episodes'] = episodes
    
    # tasks.jsonl ë¡œë“œ (ìˆì„ ê²½ìš°)
    tasks_path = meta_dir / "tasks.jsonl"
    if tasks_path.exists():
        tasks = []
        with open(tasks_path, 'r') as f:
            for line in f:
                tasks.append(json.loads(line))
        metadata['tasks'] = tasks
    
    # stats.json ë¡œë“œ (ìˆì„ ê²½ìš°)
    stats_path = meta_dir / "stats.json"
    if stats_path.exists():
        with open(stats_path, 'r') as f:
            metadata['stats'] = json.load(f)
    
    # modality.json ë¡œë“œ (ìˆì„ ê²½ìš°)
    modality_path = meta_dir / "modality.json"
    if modality_path.exists():
        with open(modality_path, 'r') as f:
            metadata['modality'] = json.load(f)
    
    return metadata


def get_last_index_from_parquet(data_dir):
    """ë°ì´í„°ì…‹ì˜ ë§ˆì§€ë§‰ index ê°’ì„ ì°¾ê¸°"""
    data_dir = Path(data_dir)
    parquet_dir = data_dir / "data" / "chunk-000"
    
    parquet_files = sorted(parquet_dir.glob("*.parquet"))
    if not parquet_files:
        return -1
    
    # ë§ˆì§€ë§‰ parquet íŒŒì¼ ì½ê¸°
    last_file = parquet_files[-1]
    table = pq.read_table(last_file)
    
    # index ì»¬ëŸ¼ì˜ ë§ˆì§€ë§‰ ê°’ ë°˜í™˜
    if 'index' in table.column_names:
        index_column = table.column('index').to_pylist()
        return max(index_column) if index_column else -1
    
    return -1


def merge_datasets(data_dir1, data_dir2, output_dir, fps=None, timestamp_unit="s", timestamp_as_int=False):
    """
    ë‘ ê°œì˜ LeRobot ë°ì´í„°ì…‹ì„ ë³‘í•©
    
    Args:
        data_dir1: ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        data_dir2: ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ
        fps: FPS (íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë°ìš©, Noneì´ë©´ ë¦¬íƒ€ì´ë° ì•ˆ í•¨)
        timestamp_unit: íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ìœ„ ("s", "ms", "us", "ns")
        timestamp_as_int: íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ int64ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
    """
    data_dir1 = Path(data_dir1)
    data_dir2 = Path(data_dir2)
    output_dir = Path(output_dir)
    
    print("=" * 60)
    print("LeRobot ë°ì´í„°ì…‹ ë³‘í•© ì‹œì‘")
    print("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / "data" / "chunk-000").mkdir(parents=True, exist_ok=True)
    (output_dir / "meta").mkdir(parents=True, exist_ok=True)
    (output_dir / "videos" / "chunk-000").mkdir(parents=True, exist_ok=True)
    
    # 1. ë°ì´í„°ì…‹ í™•ì¸
    print("\n[1/5] ë°ì´í„°ì…‹ í™•ì¸ ì¤‘...")
    print(f"  - Dataset 1: {data_dir1}")
    parquet_files1 = list((data_dir1 / "data" / "chunk-000").glob("*.parquet"))
    if not parquet_files1:
        raise FileNotFoundError(f"No parquet files found in {data_dir1 / 'data' / 'chunk-000'}")
    print(f"    âœ“ {len(parquet_files1)}ê°œ parquet íŒŒì¼")
    
    print(f"  - Dataset 2: {data_dir2}")
    parquet_files2 = list((data_dir2 / "data" / "chunk-000").glob("*.parquet"))
    if not parquet_files2:
        raise FileNotFoundError(f"No parquet files found in {data_dir2 / 'data' / 'chunk-000'}")
    print(f"    âœ“ {len(parquet_files2)}ê°œ parquet íŒŒì¼")
    
    # 2. ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print("\n[2/5] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")
    meta1 = load_metadata(data_dir1 / "meta")
    meta2 = load_metadata(data_dir2 / "meta")
    
    # 3. Parquet íŒŒì¼ ë³µì‚¬
    print("\n[3/5] Parquet íŒŒì¼ ë³µì‚¬ ì¤‘...")
    
    num_episodes1 = meta1['info']['total_episodes']
    num_frames1 = meta1['info']['total_frames']
    
    # Dataset1ì˜ ë§ˆì§€ë§‰ index ì°¾ê¸°
    last_index1 = get_last_index_from_parquet(data_dir1)
    print(f"  ğŸ“Œ Dataset1ì˜ ë§ˆì§€ë§‰ index: {last_index1}")
    
    # Dataset1ì˜ parquet íŒŒì¼ ë³µì‚¬ ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì •
    src1_parquet = data_dir1 / "data" / "chunk-000"
    dst_parquet = output_dir / "data" / "chunk-000"
    
    parquet_files1 = sorted(src1_parquet.glob("*.parquet"))
    
    for pf in parquet_files1:
        # Parquet íŒŒì¼ ì½ê¸°
        table = pq.read_table(pf)
        df = table.to_pandas()

        # íƒœìŠ¤í¬ 0
        # task_cols = detect_task_cols(df)
        # for col in task_cols:
        #     df[col] = [0] * len(df)

        if fps is not None:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ì°¾ê¸°
            ts_cols = detect_timestamp_cols(df)
            if ts_cols:
                # ê° íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ë¦¬íƒ€ì´ë°
                for col in ts_cols:
                    df[col] = make_uniform(len(df), fps, timestamp_unit, base=0.0, as_int=timestamp_as_int)
            
        # ë‹¤ì‹œ parquetë¡œ ì €ì¥
        table = pa.Table.from_pandas(df)
        pq.write_table(table, dst_parquet / pf.name)
    print(f"  âœ“ Dataset1: {len(parquet_files1)}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ (íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë°)")
    
    # Dataset2ì˜ parquet íŒŒì¼ ë³µì‚¬ ë° ë‚´ë¶€ ë°ì´í„° ì¡°ì •
    src2_parquet = data_dir2 / "data" / "chunk-000"
    parquet_files2 = sorted(src2_parquet.glob("*.parquet"))
    
    for pf in parquet_files2:
        # Parquet íŒŒì¼ ì½ê¸°
        table = pq.read_table(pf)
        df = table.to_pandas()
        
        # episode_indexì™€ index ì¡°ì •
        if 'episode_index' in df.columns:
            df['episode_index'] = df['episode_index'] + num_episodes1
        if 'index' in df.columns:
            df['index'] = df['index'] + last_index1 + 1

        # íƒœìŠ¤í¬ 1
        # task_cols = detect_task_cols(df)
        # for col in task_cols:
        #     df[col] = [1] * len(df)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë° (fpsê°€ ì§€ì •ëœ ê²½ìš°)
        if fps is not None:
            ts_cols = detect_timestamp_cols(df)
            if ts_cols:
                for col in ts_cols:
                    df[col] = make_uniform(len(df), fps, timestamp_unit, base=0.0, as_int=timestamp_as_int)
        
        # ìƒˆë¡œìš´ íŒŒì¼ëª… ìƒì„±
        new_name = pf.name
        match = re.search(r'episode_(\d+)', new_name)
        if match:
            ep_num = int(match.group(1))
            new_ep_num = ep_num + num_episodes1
            old_pattern = match.group(0)
            new_pattern = f"episode_{new_ep_num:0{len(match.group(1))}d}"
            new_name = new_name.replace(old_pattern, new_pattern)
        
        # ìˆ˜ì •ëœ parquet íŒŒì¼ ì €ì¥
        table = pa.Table.from_pandas(df)
        pq.write_table(table, dst_parquet / new_name)
    
    if fps is not None:
        print(f"  âœ“ Dataset2: {len(parquet_files2)}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ (episode_index, index, íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì •)")
    else:
        print(f"  âœ“ Dataset2: {len(parquet_files2)}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ (episode_index ë° index ì¡°ì •)")
    print(f"  âœ“ ì´ {len(parquet_files1) + len(parquet_files2)}ê°œ íŒŒì¼")
    
    # 4. ë©”íƒ€ë°ì´í„° ë³‘í•©
    print("\n[4/5] ë©”íƒ€ë°ì´í„° ë³‘í•© ì¤‘...")
    merged_meta = {}
    
    # info.json ë³‘í•©
    merged_meta['info'] = meta1['info'].copy()
    merged_meta['info']['total_episodes'] = num_episodes1 + meta2['info']['total_episodes']
    merged_meta['info']['total_frames'] = num_frames1 + meta2['info']['total_frames']
    merged_meta['info']['total_tasks'] = meta1['info'].get('total_tasks', 0) + meta2['info'].get('total_tasks', 0)
    merged_meta['info']['total_videos'] = meta1['info'].get('total_videos', 0) + meta2['info'].get('total_videos', 0)
    merged_meta['info']['total_chunks'] = 1  # í•˜ë‚˜ì˜ chunkë¡œ ë³‘í•©
    
    # splits ì •ë³´ ì—…ë°ì´íŠ¸
    if 'splits' in merged_meta['info']:
        for split_name, split_value in merged_meta['info']['splits'].items():
            # splitsê°€ "0:100" í˜•íƒœì˜ ë¬¸ìì—´ì¸ ê²½ìš°
            if isinstance(split_value, str) and ':' in split_value:
                parts = split_value.split(':')
                if len(parts) == 2:
                    start, end = int(parts[0]), int(parts[1])
                    # train splitì€ 0ë¶€í„° ì „ì²´ ì—í”¼ì†Œë“œê¹Œì§€
                    if split_name == 'train':
                        merged_meta['info']['splits'][split_name] = f"0:{merged_meta['info']['total_episodes']}"
            # splitsê°€ ìˆ«ìì¸ ê²½ìš°
            elif isinstance(split_value, (int, float)):
                if split_name == 'train':
                    merged_meta['info']['splits'][split_name] = merged_meta['info']['total_episodes']
    
    # episodes.jsonl ë³‘í•©
    if 'episodes' in meta1 and 'episodes' in meta2:
        merged_episodes = meta1['episodes'].copy()
        for ep in meta2['episodes']:
            ep_adjusted = ep.copy()
            ep_adjusted['episode_index'] = ep['episode_index'] + num_episodes1
            merged_episodes.append(ep_adjusted)
        merged_meta['episodes'] = merged_episodes
    
    # tasks.jsonl ë³‘í•© (ìˆì„ ê²½ìš°)
    merged_meta['tasks'] = meta1['tasks']
    # if 'tasks' in meta1 and 'tasks' in meta2:
    #     merged_meta['tasks'] = meta1['tasks'] + meta2['tasks']
    # elif 'tasks' in meta1:
    #     merged_meta['tasks'] = meta1['tasks']
    # elif 'tasks' in meta2:
    #     merged_meta['tasks'] = meta2['tasks']
    
    # stats.jsonì€ ìƒˆë¡œ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ì¼ë‹¨ dataset1ì˜ ê²ƒì„ ì‚¬ìš©
    if 'stats' in meta1:
        merged_meta['stats'] = meta1['stats']
        print("  âš  stats.jsonì€ dataset1ì˜ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•„ìš”ì‹œ ì¬ê³„ì‚°í•˜ì„¸ìš”.")
    
    # modality.json ë³‘í•©
    if 'modality' in meta1 and 'modality' in meta2:
        # ë‘ ë°ì´í„°ì…‹ì˜ modalityê°€ ë™ì¼í•œì§€ í™•ì¸
        if meta1['modality'] == meta2['modality']:
            merged_meta['modality'] = meta1['modality']
        else:
            print("  âš  ë‘ ë°ì´í„°ì…‹ì˜ modalityê°€ ë‹¤ë¦…ë‹ˆë‹¤. dataset1ì˜ modalityë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            merged_meta['modality'] = meta1['modality']
    elif 'modality' in meta1:
        merged_meta['modality'] = meta1['modality']
    elif 'modality' in meta2:
        merged_meta['modality'] = meta2['modality']
    
    # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
    print("\n[5/5] ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    with open(output_dir / "meta" / "info.json", 'w') as f:
        json.dump(merged_meta['info'], f, indent=2)
    print(f"  âœ“ info.json ì €ì¥")
    
    if 'episodes' in merged_meta:
        with open(output_dir / "meta" / "episodes.jsonl", 'w') as f:
            for ep in merged_meta['episodes']:
                f.write(json.dumps(ep) + '\n')
        print(f"  âœ“ episodes.jsonl ì €ì¥")
    
    if 'tasks' in merged_meta:
        with open(output_dir / "meta" / "tasks.jsonl", 'w') as f:
            for task in merged_meta['tasks']:
                f.write(json.dumps(task) + '\n')
        print(f"  âœ“ tasks.jsonl ì €ì¥")
    
    if 'stats' in merged_meta:
        with open(output_dir / "meta" / "stats.json", 'w') as f:
            json.dump(merged_meta['stats'], f, indent=2)
        print(f"  âœ“ stats.json ì €ì¥")
    
    if 'modality' in merged_meta:
        with open(output_dir / "meta" / "modality.json", 'w') as f:
            json.dump(merged_meta['modality'], f, indent=2)
        print(f"  âœ“ modality.json ì €ì¥")
    
    # videos ë³µì‚¬
    print("\n[6/6] ë¹„ë””ì˜¤ íŒŒì¼ ë³µì‚¬ ì¤‘...")
    video_src1 = data_dir1 / "videos" / "chunk-000"
    video_src2 = data_dir2 / "videos" / "chunk-000"
    video_dst = output_dir / "videos" / "chunk-000"
    
    # dataset1ì˜ ë¹„ë””ì˜¤ ë³µì‚¬
    video_count1 = 0
    if video_src1.exists():
        for video_file in video_src1.glob("*.mp4"):
            shutil.copy2(video_file, video_dst / video_file.name)
            video_count1 += 1
        print(f"  âœ“ Dataset1: {video_count1}ê°œ ë¹„ë””ì˜¤ ë³µì‚¬ ì™„ë£Œ")
    
    # dataset2ì˜ ë¹„ë””ì˜¤ ë³µì‚¬ (ì´ë¦„ ì¡°ì •)
    video_count2 = 0
    if video_src2.exists():
        for video_file in video_src2.glob("*.mp4"):
            new_name = video_file.name
            
            # episode_XXXX í˜•íƒœì˜ ë²ˆí˜¸ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ì•„ì„œ ì¡°ì •
            # ì˜ˆ: episode_0093.mp4 -> episode_0193.mp4 (100 ì¶”ê°€)
            match = re.search(r'episode_(\d+)', new_name)
            if match:
                ep_num = int(match.group(1))
                new_ep_num = ep_num + num_episodes1
                # ì›ë˜ ìë¦¿ìˆ˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë²ˆí˜¸ êµì²´
                old_pattern = match.group(0)  # episode_0093
                new_pattern = f"episode_{new_ep_num:0{len(match.group(1))}d}"
                new_name = new_name.replace(old_pattern, new_pattern)
            
            shutil.copy2(video_file, video_dst / new_name)
            video_count2 += 1
        print(f"  âœ“ Dataset2: {video_count2}ê°œ ë¹„ë””ì˜¤ ë³µì‚¬ ì™„ë£Œ (episode ë²ˆí˜¸ ì¡°ì •)")
    
    print(f"  âœ“ ì´ {video_count1 + video_count2}ê°œ ë¹„ë””ì˜¤")
    
    print("\n" + "=" * 60)
    print("âœ… ë³‘í•© ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š ë³‘í•© ê²°ê³¼:")
    print(f"  - ì´ ì—í”¼ì†Œë“œ: {merged_meta['info']['total_episodes']}")
    print(f"  - ì´ í”„ë ˆì„: {merged_meta['info']['total_frames']}")
    print(f"  - Parquet íŒŒì¼: {len(parquet_files1) + len(parquet_files2)}ê°œ")
    print(f"  - ë¹„ë””ì˜¤ íŒŒì¼: {video_count1 + video_count2}ê°œ")
    print(f"  - ì¶œë ¥ ê²½ë¡œ: {output_dir}")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="LeRobot ë°ì´í„°ì…‹ ë³‘í•©")
    parser.add_argument("--data_dir1", type=str, required=True, help="ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--data_dir2", type=str, required=True, help="ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, required=True, help="ì¶œë ¥ ë°ì´í„°ì…‹ ê²½ë¡œ")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë° ì˜µì…˜
    parser.add_argument("--fps", type=float, default=None, help="íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬íƒ€ì´ë°ì„ ìœ„í•œ FPS (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë¦¬íƒ€ì´ë° ì•ˆ í•¨)")
    parser.add_argument("--timestamp_unit", choices=["s", "ms", "us", "ns"], default="s", 
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ìœ„ (ê¸°ë³¸ê°’: s)")
    parser.add_argument("--timestamp_as_int", action="store_true", 
                        help="íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ int64ë¡œ ì €ì¥ (ê¸°ë³¸ê°’: float64)")
    
    args = parser.parse_args()
    
    merge_datasets(
        args.data_dir1, 
        args.data_dir2, 
        args.output_dir,
        fps=args.fps,
        timestamp_unit=args.timestamp_unit,
        timestamp_as_int=args.timestamp_as_int
    )
